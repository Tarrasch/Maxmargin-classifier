\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}

\title{Duality Forumlation of Max-Margin Classifier}
\author{Arash Rouhani}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a short mathematical explenation of how to formulate
the Max-Margin Classifier problem in its dual form.
I go through the derivation step by step by formulating the lagrangian
function and then performing sensible steps to finally reach the desired dual formulation. 

\end{abstract}

\section{Primal formulation}
I skip formulating the primal here, and instead refer to \url{http://www.cse.chalmers.se/edu/year/2011/course/TDA206_Discrete_Optimization/hw4/exercise4.pdf}
\section{Lagrangian function}
There will be as always 3 kinds of terms, coming from either the \textit{the objective function},\textit{a constraint} or \textit{a variable constraint}.
% \begin{equation}
% L(w, b, \xi, \mu, \nu) = \frac{1}{2} ||w||^2_2 + \sum\limits_{i=1}^n {C\xi_i - \mu_iy_i(w^Tx_i - b + \xi_i - 1) - \nu_i\xi_i}
% \end{equation}
% Can simply be rewritten to
\begin{equation}
L(w, b, \xi, \mu, \nu) = \frac{1}{2} ||w||^2_2 + \sum\limits_{i=1}^n {C\xi_i + \mu_i(y_ib + 1 - y_iw^Tx_i - \xi_i) - \nu_i\xi_i}
\end{equation}
We now derivativs for each variable-type and get an equation. There are 3 'types' of variables here, so we will get 3 equations.

\subsection{Derivative with respect to w}
  \begin{equation}
    \frac{dL}{dw} = \textbf{0} = w - \sum\limits_{i=1}^n {\mu_i y_i x_i}
  \end{equation}
% WRONG - %  Here $x_i$ means the sum of the values in the \textbf{vector} $x_i$.
\subsection{Derivative with respect to b}
  \begin{equation}
    \frac{dL}{db} = 0 = \sum\limits_{i=1}^n {\mu_i y_i}
  \end{equation}
\subsection{Derivative with respect to $\xi$}
  \begin{equation}
    \frac{dL}{db} = 0 = C - \mu_i - \nu_i
  \end{equation}

\section{Reducing the lagrangian by substitution}
First we substitute (4) which is clearly eliminating the $\xi$ terms.
\begin{equation}
  L(w, b, \mu) = \frac{1}{2} ||w||^2_2 + \sum\limits_{i=1}^n {\mu_i(y_ib + 1 - y_iw^Tx_i)}
\end{equation}
Let's rewrite the function using $ ||w||^2_2 = w^Tw $ and other simple rewritings.
\begin{equation}
  L(w, b, \mu) = w^T(\frac{1}{2}w - \sum\limits_{i=1}^n {\mu_iy_ix_i}) + b\sum\limits_{i=1}^n {\mu_iy_i} + \sum\limits_{i=1}^n {\mu_i}
\end{equation}
Now we use (2) and (3) in order to reduce once more.
\begin{equation}
  L(w, \mu) = \sum\limits_{i=1}^n {\mu_i} - \frac{1}{2} w^T \sum\limits_{i=1}^n{\mu_iy_ix_i} 
\end{equation}
Now we use (2) again to rewrite $w^T$, finally eliminating all free variables we had in the primal.
\begin{equation}
  L(\mu) = \sum\limits_{i=1}^n {\mu_i} - \frac{1}{2} \sum\limits_{j=1}^n{\mu_jy_jx_j^T} \sum\limits_{i=1}^n{\mu_iy_ix_i} 
\end{equation}
Now we finally rewrite the lagrangian to it's well-recognized form.
\begin{equation}
  L(\mu) = \sum\limits_{i=1}^n {\mu_i} - \frac{1}{2} \sum\limits_{i=1}^n \sum\limits_{j=1}^n{\mu_i\mu_jy_iy_jx_i^Tx_j}
\end{equation}
\section{Finding the constraints}
So far we've only retrieved the objective function out of the Lagrangian, now the constraints remain.
Firstly we must notice that we've always known that $ \mu,\nu > 0 $,
dual variables are always $\ge 0$ (otherwise it would be
a \textit{gain} to violate the constraints).
The rest of the information we are going to work with is the equalities
we got after the derivations in chapter 2.


The first constraint we take is simply equation (3). The second constraint I claim is
$$ 0 \le \mu_i \le C $$
$$ 1 \le     i \le n $$ 

$\mu_i \ge 0$ Holds by definition. $C \ge \mu_i$ holds as of (4) by regulating $\nu \ge 0$
in $\mu_i = C - \nu_i$. It's clear that we get the constraint claimed above.

\section{Conclusions}
We got the exact result we wanted.

\end{document}
